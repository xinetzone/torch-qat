
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>概述 &#8212; torch_qat 0.0.1rc1 文档</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=4fea3ae4a0a4ba949df0a89bbae557cc2ce861ae" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/default.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="_static/favicon.jpg"/>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <link rel="next" title="PTQ 与 QAT 量化实践（PyTorch）" href="best-practice.html" />
    <link rel="prev" title="PyTorch 量化教程" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">torch_qat 0.0.1rc1 文档</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   概述
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="best-practice.html">
   PTQ 与 QAT 量化实践（PyTorch）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="confuse.html">
   融合
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="config.html">
   配置
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="train.html">
   训练
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/xinetzone/torch-quantization/edit/main/docs/intro.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/intro.md.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> 导航
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#api">
   量化 API 概述
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eager">
     Eager 模式量化
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       动态量化
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       静态量化
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       量化感知训练
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fx">
     （原型）FX 图模式量化
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   量化 API 参考
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   量化张量
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   原生支持的后端
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   量化定制
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     量化自定义模块 API
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   量化模型准备（Eager 模式）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   最佳实践（已废弃）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   数值调试（原型）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   常见错误
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     保存和加载量化模型
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     传递非量化的张量到量化的核
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     传递量化的张量到非量化的核
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>概述</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> 导航 </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#api">
   量化 API 概述
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eager">
     Eager 模式量化
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       动态量化
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       静态量化
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       量化感知训练
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fx">
     （原型）FX 图模式量化
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   量化 API 参考
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   量化张量
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   原生支持的后端
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   量化定制
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     量化自定义模块 API
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   量化模型准备（Eager 模式）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   最佳实践（已废弃）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   数值调试（原型）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   常见错误
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     保存和加载量化模型
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     传递非量化的张量到量化的核
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     传递量化的张量到非量化的核
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>概述<a class="headerlink" href="#id1" title="永久链接至标题">#</a></h1>
<p>参考：<a class="reference external" href="https://pytorch.org/docs/master/quantization.html">量化</a></p>
<p><strong>量化</strong> 是指以比浮点精度更低的位宽执行计算和存储张量的技术。量化模型对具有整数而不是浮点值的张量执行部分或全部运算。这允许在许多硬件平台上使用更紧凑的模型表示和高性能矢量化运算。与典型的 FP32 模型相比，PyTorch 支持 INT8 量化，允许模型尺寸减少 4 倍，内存带宽需求减少 4 倍。INT8 计算的硬件支持通常比 FP32 计算快 2 到 4 倍。量化主要是加速推理的技术，只支持量化算子的前向传播。</p>
<p>PyTorch 支持多种方法来量化深度学习模型。在大多数情况下，模型是在 FP32 中训练的，然后将模型转换为 INT8。此外，PyTorch 还支持量化感知训练（quantization aware training，简称 QAT），它使用伪量化模块对正向和反向传播中的量化误差进行建模。请注意，整个计算均是浮点运算。在量化感知训练结束时，PyTorch 提供了转换函数，将训练后的模型转换成精度（precision）较低的模型。</p>
<p>在低层接口，PyTorch 提供了表示量化张量（quantized tensor）及其运算的方法。它们可以用来直接构建模型，以较低的精度执行全部或部分计算。也提供高层 API，结合 FP32 模型转换的典型工作流程，以最小的 accuracy 损失降低精度。</p>
<p>量化要求用户了解三个概念：</p>
<ul class="simple">
<li><p>量化配置（<code class="docutils literal notranslate"><span class="pre">Qconfig</span></code>）：使用 <a class="reference external" href="https://xinetzone.github.io/pytorch-book/api/pytorch/ao/quantization/generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig" title="(在 Pytorch Book)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QConfig</span></code></a> 指定权重和激活的量化方案。</p></li>
<li><p>后端：提供支持量化的内核，通常使用不同的数值。</p></li>
<li><p>量化引擎（<code class="docutils literal notranslate"><span class="pre">torch.backends.quantization.engine</span></code>）：当执行量化模型时，<code class="docutils literal notranslate"><span class="pre">qengine</span></code> 指定执行时使用哪个后端。重要的是要确保 <code class="docutils literal notranslate"><span class="pre">qengine</span></code> 与 <code class="docutils literal notranslate"><span class="pre">Qconfig</span></code> 一致。</p></li>
</ul>
<section id="api">
<h2>量化 API 概述<a class="headerlink" href="#api" title="永久链接至标题">#</a></h2>
<p>PyTorch 提供了两种不同的量化模式：Eager 模式量化和 FX 图模式量化。</p>
<p>Eager 模式量化是 beta 特性。用户需要进行融合，并手动指定量化和反量化发生的位置，而且它只支持模块而不支持函数。FX 图模式量化是 PyTorch 中新的自动量化框架，目前它是原型（prototype）特性。它通过添加对函数的支持和量化过程的自动化，对 Eager 模式量化进行了改进，尽管人们可能需要重构模型，以使模型与 FX Graph 模式量化兼容（通过 <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#module-torch.fx" title="(在 PyTorch v1.11.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.fx</span></code></a> 符号可追溯（symbolically traceable））。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>FX 图模式量化预计不会在任意可能不是 symbolically traceable 的模型工作，我们会将其集成到域库 torchvision 和用户将能够量化模型类似于支持域的库与 FX 图模式量化。对于任意的模型，我们将提供一般的指导方针，但要让它实际工作，用户可能需要熟悉 <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#module-torch.fx" title="(在 PyTorch v1.11.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.fx</span></code></a>，特别是如何使模型具有符号可追溯性。</p>
<p>新用户的量化鼓励尝试 FX 图模式量化首先，如果它不工作，用户可以尝试遵循<a class="reference external" href="https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html">使用 FX 图模式量化</a>的指导方针或回落到 Eager 模式量化。</p>
</div>
<p>支持三种类型的量化：</p>
<ol class="simple">
<li><p>动态量化（dynamic quantization）：通过读取/存储在浮点数中的激活（activation）以量化权重（weight），并量化用于计算。</p></li>
<li><p>静态量化（static quantization）：权重量化，激活量化，需要训练后校准。</p></li>
<li><p>静态量化感知训练（static quantization aware training）：权重量化，激活量化，训练过程中的量化数值建模。</p></li>
</ol>
<section id="eager">
<h3>Eager 模式量化<a class="headerlink" href="#eager" title="永久链接至标题">#</a></h3>
<section id="id2">
<h4>动态量化<a class="headerlink" href="#id2" title="永久链接至标题">#</a></h4>
<p>这是最简单的量化形式，其中权值（weight）提前量化，但在推理期间动态量化激活（activation）。这用于模型执行时由从内存中加载权重控制而不是计算矩阵乘法的情况。这适用于小批量的 LSTM 和 Transformer 类型模型。</p>
<p class="rubric">示意图</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 原始 model</span>
<span class="c1"># 所有的张量和计算都是浮点数</span>
<span class="n">previous_layer_fp32</span> <span class="o">--</span> <span class="n">linear_fp32</span> <span class="o">--</span> <span class="n">activation_fp32</span> <span class="o">--</span> <span class="n">next_layer_fp32</span>
                 <span class="o">/</span>
<span class="n">linear_weight_fp32</span>

<span class="c1"># 动态量化 model</span>
<span class="c1"># linear 和 LSTM 权重是 int8</span>
<span class="n">previous_layer_fp32</span> <span class="o">--</span> <span class="n">linear_int8_w_fp32_inp</span> <span class="o">--</span> <span class="n">activation_fp32</span> <span class="o">--</span> <span class="n">next_layer_fp32</span>
                     <span class="o">/</span>
   <span class="n">linear_weight_int8</span>
</pre></div>
</div>
<p class="rubric">示例</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># 定义浮点模型</span>
<span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="c1"># 创建模型实例</span>
<span class="n">model_fp32</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span>
<span class="c1"># 创建量化模型实例</span>
<span class="n">model_int8</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model_fp32</span><span class="p">,</span>  <span class="c1"># 原始模型</span>
    <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">},</span>  <span class="c1"># 一组要动态量化的层</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">)</span>  <span class="c1"># 量化权重的目标 dtype</span>

<span class="c1"># 运行模型</span>
<span class="n">input_fp32</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">model_int8</span><span class="p">(</span><span class="n">input_fp32</span><span class="p">)</span>
</pre></div>
</div>
<p>要了解更多关于动态量化的信息，请参阅<a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html">动态量化教程</a>。</p>
</section>
<section id="id3">
<h4>静态量化<a class="headerlink" href="#id3" title="永久链接至标题">#</a></h4>
<p>静态量化（static quantization）对模型的权重和激活进行量化。它在可能的情况下将激活融合到前面的层中。它需要用具有代表性的数据集进行校准，以确定激活的最佳量化参数。当内存带宽和计算空间都很重要时，通常会使用训练后量化，而 CNN 就是其典型的用例。静态量化也被称为训练后量化（Post Training Quantization）或 PTQ。</p>
<p class="rubric">示意图</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 原始模型</span>
<span class="c1"># 全部的张量和计算均在浮点上进行</span>
<span class="n">previous_layer_fp32</span> <span class="o">--</span> <span class="n">linear_fp32</span> <span class="o">--</span> <span class="n">activation_fp32</span> <span class="o">--</span> <span class="n">next_layer_fp32</span>
                    <span class="o">/</span>
    <span class="n">linear_weight_fp32</span>

<span class="c1"># 静态量化模型</span>
<span class="c1"># weights 和 activations 在 int8 上</span>
<span class="n">previous_layer_int8</span> <span class="o">--</span> <span class="n">linear_with_activation_int8</span> <span class="o">--</span> <span class="n">next_layer_int8</span>
                    <span class="o">/</span>
  <span class="n">linear_weight_int8</span>
</pre></div>
</div>
<p class="rubric">示例</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># 定义浮点模型，其中一些层可以被静态量化</span>
<span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># QuantStub 将张量从浮点转换为量化</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantStub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="c1"># DeQuantStub 将张量从量化转换为浮点</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dequant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">DeQuantStub</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 手动指定张量将在量化模型中从浮点转换为量化的位置</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># 在量化模型中手动指定张量从量化到浮点的转换位置</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># 创建模型实例</span>
<span class="n">model_fp32</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span>

<span class="c1"># 要使静态量化逻辑工作，必须将模型设置为 eval 模式</span>
<span class="n">model_fp32</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># 附加全局 qconfig，其中包含关于要附加哪种观测器的信息。</span>
<span class="c1"># 使用 &#39;fbgemm&#39; 进行端推理，使用 &#39;qnnpack&#39; 进行移动端推理。</span>
<span class="c1"># 其他量化配置，如选择对称或非对称量化和 MinMax 或 L2Norm 校准技术，可以在这里指定。</span>
<span class="n">model_fp32</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">)</span>

<span class="c1"># 融合 activation 到前面的层，在适用的地方。</span>
<span class="c1"># 这需要根据模型架构手动完成。</span>
<span class="c1"># 常见的融合包括 `conv + relu` 和 `conv + batchnorm + relu`</span>
<span class="n">model_fp32_fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">model_fp32</span><span class="p">,</span> <span class="p">[[</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">]])</span>

<span class="c1"># 准备静态量化模型。</span>
<span class="c1"># 这将在模型中插入观测器，用于在校准期间观测激活（activation）张量。</span>
<span class="n">model_fp32_prepared</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model_fp32_fused</span><span class="p">)</span>

<span class="c1"># 校准准备的模型，以确定量化参数的激活在现实世界的设置，校准具有代表性的数据集</span>
<span class="n">input_fp32</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">model_fp32_prepared</span><span class="p">(</span><span class="n">input_fp32</span><span class="p">)</span>

<span class="c1"># 将观测到的模型转换为量化模型。</span>
<span class="c1"># 这做了几件事：</span>
<span class="c1"># 量化权重，计算和存储每个激活张量要使用的尺度（scale）和偏差（bias）值，并用量化实现替换关键算子。</span>
<span class="n">model_int8</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model_fp32_prepared</span><span class="p">)</span>

<span class="c1"># 运行模型，相关的计算将在 int8 中发生</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">model_int8</span><span class="p">(</span><span class="n">input_fp32</span><span class="p">)</span>
</pre></div>
</div>
<p>要了解更多关于静态量化的信息，请参阅<a class="reference external" href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">静态量化教程</a>。</p>
</section>
<section id="id4">
<h4>量化感知训练<a class="headerlink" href="#id4" title="永久链接至标题">#</a></h4>
<p>与其他量化方法相比，量化感知训练（Quantization Aware Training）在训练过程中模拟量化的效果，允许更高的 accuracy。在训练过程中，所有的计算都是在浮点上进行的，使用 <code class="docutils literal notranslate"><span class="pre">fake_quant</span></code> 模块通过夹紧和舍入的方式对量化效果进行建模，模拟 INT8 的效果。模型转换后，权值和激活被量化，激活在可能的情况下被融合到前一层。它通常与 CNN 一起使用，与静态量化相比具有更高的 accuracy。量化感知训练也被称为 QAT。</p>
<p class="rubric">示意图</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 原始模型</span>
<span class="c1"># 全部张量和计算均在浮点上</span>
<span class="n">previous_layer_fp32</span> <span class="o">--</span> <span class="n">linear_fp32</span> <span class="o">--</span> <span class="n">activation_fp32</span> <span class="o">--</span> <span class="n">next_layer_fp32</span>
                      <span class="o">/</span>
    <span class="n">linear_weight_fp32</span>

<span class="c1"># 在训练过程中使用 fake_quants 建模量化数值</span>
<span class="n">previous_layer_fp32</span> <span class="o">--</span> <span class="n">fq</span> <span class="o">--</span> <span class="n">linear_fp32</span> <span class="o">--</span> <span class="n">activation_fp32</span> <span class="o">--</span> <span class="n">fq</span> <span class="o">--</span> <span class="n">next_layer_fp32</span>
                           <span class="o">/</span>
   <span class="n">linear_weight_fp32</span> <span class="o">--</span> <span class="n">fq</span>

<span class="c1"># 量化模型</span>
<span class="c1"># weights 和 activations 在 int8 上</span>
<span class="n">previous_layer_int8</span> <span class="o">--</span> <span class="n">linear_with_activation_int8</span> <span class="o">--</span> <span class="n">next_layer_int8</span>
                     <span class="o">/</span>
   <span class="n">linear_weight_int8</span>
</pre></div>
</div>
<p class="rubric">示例</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># 定义浮点模型，其中一些层可以受益于 QAT</span>
<span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># QuantStub 将张量从浮点转换为量化</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantStub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="c1"># DeQuantStub 将张量从量化转换为浮点</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dequant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">DeQuantStub</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="c1"># 创建模型实例</span>
<span class="n">model_fp32</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span>

<span class="c1"># 模型必须设置为训练模式，以便 QAT 逻辑工作</span>
<span class="n">model_fp32</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># 附加全局 qconfig，其中包含关于要附加哪种观测器的信息。</span>
<span class="c1"># 使用 &#39;fbgemm&#39; 进行服务器端推理，使用 &#39;qnnpack&#39; 进行移动端推理。</span>
<span class="c1"># 其他量化配置，如选择对称或非对称量化和 MinMax 或 L2Norm 校准技术，可以在这里指定。</span>
<span class="n">model_fp32</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">)</span>

<span class="c1"># 将激活融合到前面的层，在适用的情况下，这需要根据模型体系结构手工完成</span>
<span class="n">model_fp32_fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">model_fp32</span><span class="p">,</span>
                                                   <span class="p">[[</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="s1">&#39;bn&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">]])</span>

<span class="c1"># 为 QAT 准备模型。</span>
<span class="c1"># 这将在模型中插入观测者和 fake_quants，它们将在校准期间观测权值和激活张量。</span>
<span class="n">model_fp32_prepared</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare_qat</span><span class="p">(</span><span class="n">model_fp32_fused</span><span class="p">)</span>

<span class="c1"># 运行训练循环（没有显示）</span>
<span class="n">training_loop</span><span class="p">(</span><span class="n">model_fp32_prepared</span><span class="p">)</span>

<span class="c1"># 将观测到的模型转换为量化模型。这有几件事：</span>
<span class="c1"># 量化权重，计算和存储用于每个激活张量的尺度（scale）和偏差（bias）值，</span>
<span class="c1"># 在适当的地方融合模块，并用量化实现替换关键算子。</span>
<span class="n">model_fp32_prepared</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">model_int8</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model_fp32_prepared</span><span class="p">)</span>

<span class="c1"># 运行模型，相关的计算将在 int8 中发生</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">model_int8</span><span class="p">(</span><span class="n">input_fp32</span><span class="p">)</span>
</pre></div>
</div>
<p>要了解更多关于量化意识训练的信息，请参阅 <a class="reference external" href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">QAT 教程</a>。</p>
</section>
</section>
<section id="fx">
<h3>（原型）FX 图模式量化<a class="headerlink" href="#fx" title="永久链接至标题">#</a></h3>
<p>FX 图（Graph）模式支持的量化类型可以分为两种方式：</p>
<ol class="simple">
<li><p>PTQ：训练后进行量化，根据样本校准数据计算量化参数</p></li>
<li><p>QAT：在训练过程中模拟量化，以便利用训练数据与模型一起学习量化参数</p></li>
</ol>
<p>然后这两种可能包括以下任何一种或所有的类型：</p>
<ul class="simple">
<li><p>仅权重量化（Weight Only Quantization）：只有权重是静态量化的</p></li>
<li><p>动态量化（Dynamic Quantization）：权重静态量化，激活动态量化</p></li>
<li><p>静态量化（Static Quantization）：权重和激活都是静态量化的</p></li>
</ul>
<p>这两种分类方式是独立的，所以理论上我们可以有 6 种不同的量化方式。</p>
<p>FX 图模式量化中支持的量化类型有：</p>
<ul class="simple">
<li><p>Post Training Quantization</p>
<ul>
<li><p>Weight Only Quantization</p></li>
<li><p>Dynamic Quantization</p></li>
<li><p>Static Quantization</p></li>
</ul>
</li>
<li><p>Quantization Aware Training</p>
<ul>
<li><p>Static Quantization</p></li>
</ul>
</li>
</ul>
<p>在训练后量化中有多种量化类型（仅权重、动态和静态），配置是通过 <code class="docutils literal notranslate"><span class="pre">qconfig_dict</span> </code> （<code class="docutils literal notranslate"><span class="pre">prepare_fx</span></code> 函数的参数）完成的。</p>
<p class="rubric">示例</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.quantization.quantize_fx</span> <span class="k">as</span> <span class="nn">quantize_fx</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">model_fp</span> <span class="o">=</span> <span class="n">UserModel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># post training dynamic/weight_only quantization</span>
<span class="c1">#</span>

<span class="c1"># we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model</span>
<span class="n">model_to_quantize</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model_fp</span><span class="p">)</span>
<span class="n">model_to_quantize</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_dynamic_qconfig</span><span class="p">}</span>
<span class="c1"># prepare</span>
<span class="n">model_prepared</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">prepare_fx</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
<span class="c1"># no calibration needed when we only have dynamici/weight_only quantization</span>
<span class="c1"># quantize</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># post training static quantization</span>
<span class="c1">#</span>

<span class="n">model_to_quantize</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model_fp</span><span class="p">)</span>
<span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="s1">&#39;qnnpack&#39;</span><span class="p">)}</span>
<span class="n">model_to_quantize</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># prepare</span>
<span class="n">model_prepared</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">prepare_fx</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
<span class="c1"># calibrate (not shown)</span>
<span class="c1"># quantize</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># quantization aware training for static quantization</span>
<span class="c1">#</span>

<span class="n">model_to_quantize</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model_fp</span><span class="p">)</span>
<span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s1">&#39;qnnpack&#39;</span><span class="p">)}</span>
<span class="n">model_to_quantize</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="c1"># prepare</span>
<span class="n">model_prepared</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">prepare_qat_fx</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
<span class="c1"># training loop (not shown)</span>
<span class="c1"># quantize</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># fusion</span>
<span class="c1">#</span>
<span class="n">model_to_quantize</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model_fp</span><span class="p">)</span>
<span class="n">model_fused</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">fuse_fx</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">)</span>
</pre></div>
</div>
<p>有关 FX 图模式量化的更多信息，请参阅以下教程：</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html">User Guide on Using FX Graph Mode Quantization</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html">FX Graph Mode Post Training Static Quantization</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html">FX Graph Mode Post Training Dynamic Quantization</a></p></li>
</ul>
</section>
</section>
<section id="id5">
<h2>量化 API 参考<a class="headerlink" href="#id5" title="永久链接至标题">#</a></h2>
<p><a class="reference external" href="https://pytorch.org/docs/master/quantization-support.html">量化 API 参考</a>包含了量化 API 的文档，例如量化传递、量化张量操作以及支持的量化模块和函数。</p>
</section>
<section id="id6">
<h2>量化张量<a class="headerlink" href="#id6" title="永久链接至标题">#</a></h2>
<p>PyTorch 支持逐张量和逐通道的非对称线性量化。逐张量意味着张量内的所有值都以相同的方式缩放。逐通道意味着对于每个维度，通常是张量的通道维度，张量中的值被不同的值缩放和偏移（实际上，缩放和偏移变成了向量）。这使得将张量转换为量化值的误差更小。</p>
<p>映射是通过使用转换浮点张量来执行的：</p>
<div class="math notranslate nohighlight">
\[
Q(x, \text{scale}, \text{zero_point}) = \text{round}(\frac{x}{\text{scale}} + \text{zero_point})
\]</div>
<p>请注意，我们确保浮点数中的零点在量化后不会出现错误，从而确保 padding 之类的运算不会导致额外的量化误差。</p>
<p>为了在 PyTorch 中进行量化，需要能够用张量表示量化的数据。量化张量允许存储量化数据（表示为 int8/uint8/int32）以及量化参数，如 <code class="docutils literal notranslate"><span class="pre">scale</span></code> 和 <code class="docutils literal notranslate"><span class="pre">zero_point</span></code>。量化张量（Quantized Tensor）除了允许以量化格式对数据进行序列化外，还允许许多有用的运算使量化算术变得简单。</p>
</section>
<section id="id7">
<h2>原生支持的后端<a class="headerlink" href="#id7" title="永久链接至标题">#</a></h2>
<p>今天，PyTorch 支持以下后端来高效地运行量化算子：</p>
<ul class="simple">
<li><p>支持 AVX2 或更高版本的 x86 CPU（如果没有 AVX2，一些运算会低效实现），通过 <a class="reference external" href="https://github.com/pytorch/FBGEMM">fbgemm</a></p></li>
<li><p>ARM CPU（通常在移动/嵌入式设备中找到），通过 <a class="reference external" href="https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu/qnnpack">qnnpack</a></p></li>
</ul>
<p>相应的实现会根据 PyTorch 构建模式自动选择，不过用户可以通过将 <code class="docutils literal notranslate"><span class="pre">torch.backends.quantization.engine</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">fbgemm</span></code> 或 <code class="docutils literal notranslate"><span class="pre">qnnpack</span></code> 来覆盖这个选项。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>目前 PyTorch 还没有在 CUDA 上提供量化的算子实现——这是未来工作的方向。将模型移到 CPU 上，以测试量化的功能。</p>
<p>量化感知训练（通过 <code class="docutils literal notranslate"><span class="pre">FakeQuantize</span></code>，它模拟 fp32 中的量化数值）支持 CPU 和 CUDA。</p>
</div>
<p>在准备量化模型时，必须确保 <code class="docutils literal notranslate"><span class="pre">qconfig</span></code> 和用于量化计算的引擎与将在其上执行模型的后端匹配。<code class="docutils literal notranslate"><span class="pre">qconfig</span></code> 控制量化传递期间使用的观测器类型。当对线性和卷积函数和模块进行权重打包时，<code class="docutils literal notranslate"><span class="pre">qengine</span></code> 控制是使用 <code class="docutils literal notranslate"><span class="pre">fbgemm</span></code> 还是 <code class="docutils literal notranslate"><span class="pre">qnnpack</span></code> 特定的打包函数。例如：</p>
<p><code class="docutils literal notranslate"><span class="pre">fbgemm</span></code> 的默认设置：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the qconfig for PTQ</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">)</span>
<span class="c1"># or, set the qconfig for QAT</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">)</span>
<span class="c1"># set the qengine to control weight packing</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">engine</span> <span class="o">=</span> <span class="s1">&#39;fbgemm&#39;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">qnnpack</span></code> 的默认设置：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the qconfig for PTQ</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="s1">&#39;qnnpack&#39;</span><span class="p">)</span>
<span class="c1"># or, set the qconfig for QAT</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s1">&#39;qnnpack&#39;</span><span class="p">)</span>
<span class="c1"># set the qengine to control weight packing</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">engine</span> <span class="o">=</span> <span class="s1">&#39;qnnpack&#39;</span>
</pre></div>
</div>
</section>
<section id="id8">
<h2>量化定制<a class="headerlink" href="#id8" title="永久链接至标题">#</a></h2>
<p>虽然提供了基于观测到的张量数据选择 scale 因子和 bias 的观测者的默认实现，但开发人员可以提供自己的量化函数。量化可以有选择地应用于模型的不同部分，也可以针对模型的不同部分进行不同的配置。</p>
<p>PyTorch 还为 <code class="xref py py-func docutils literal notranslate"><span class="pre">conv2d()</span></code>、<code class="xref py py-func docutils literal notranslate"><span class="pre">conv3d()</span></code> 和 <code class="xref py py-func docutils literal notranslate"><span class="pre">linear()</span></code> 提供了逐通道量化的支持。</p>
<p>量化工作流通过添加（例如添加观测者作为 <code class="docutils literal notranslate"><span class="pre">.observer</span></code> 子模块）或替换（例如转换 <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> 为 <code class="docutils literal notranslate"><span class="pre">nn.quantized.Conv2d</span></code>）的子模块。这意味着模型在整个过程中保持常规的基于 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 的实例，因此可以与其他 PyTorch API 一起工作。</p>
<section id="id9">
<h3>量化自定义模块 API<a class="headerlink" href="#id9" title="永久链接至标题">#</a></h3>
<p>Eager 模式和 FX 图模式量化 API 都为用户提供了钩子，用户可以通过自定义的方式指定量化模块，并使用用户定义的逻辑进行观测和量化。用户需要指定：</p>
<ol class="simple">
<li><p>源 fp32 模块的 Python 类型（存在于模型中）</p></li>
<li><p>被观测模块的 Python 类型（由用户提供）。这个模块需要定义 <code class="xref py py-func docutils literal notranslate"><span class="pre">from_float()</span></code> 函数，它定义了如何从原始 fp32 模块创建观测到的模块。</p></li>
<li><p>量化模块的 Python 类型（由用户提供）。这个模块需要定义 <code class="xref py py-func docutils literal notranslate"><span class="pre">from_observed()</span></code> 函数，该函数定义如何从被观测模块创建量化的模块。</p></li>
<li><p>上面描述的 (1)、(2)、(3) 配置，传递给量化 API。</p></li>
</ol>
<p>然后，框架将执行以下操作：</p>
<ol class="simple">
<li><p>在 <code class="docutils literal notranslate"><span class="pre">prepare</span></code> 模块交换过程中，它将使用 (2) 中类的 <code class="xref py py-func docutils literal notranslate"><span class="pre">from_float()</span></code> 函数将 (1) 中指定的类型的每个模块转换为 (2) 中指定的类型</p></li>
<li><p>在 <code class="docutils literal notranslate"><span class="pre">convert</span></code> 模块交换期间，它将使用 (3) 中类的 <code class="xref py py-func docutils literal notranslate"><span class="pre">from_observed()</span></code> 函数将 (2) 中指定的类型的每个模块转换为 (3) 中指定的类型。</p></li>
</ol>
<p>目前，要求是 <code class="docutils literal notranslate"><span class="pre">ObservedCustomModule</span></code> 将有单个张量输出，并且观测者将由框架（而不是由用户）添加到该输出上。观测者将作为自定义模块实例的属性存储在 <code class="docutils literal notranslate"><span class="pre">activation_post_process</span></code> 键下。放宽这些限制可能会在未来的某个时候实现。</p>
<p class="rubric">示例</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.quantized</span> <span class="k">as</span> <span class="nn">nnq</span>
<span class="kn">import</span> <span class="nn">torch.quantization.quantize_fx</span>

<span class="c1"># 源 fp32 模块被替换</span>
<span class="k">class</span> <span class="nc">CustomModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 自定义观测者模块，由用户提供</span>
<span class="k">class</span> <span class="nc">ObservedCustomModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">float_module</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">float_module</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">)</span>
        <span class="n">observed</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">float_module</span><span class="o">.</span><span class="n">linear</span><span class="p">)</span>
        <span class="n">observed</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">float_module</span><span class="o">.</span><span class="n">qconfig</span>
        <span class="k">return</span> <span class="n">observed</span>

<span class="c1"># 自定义量化模块，由用户提供</span>
<span class="k">class</span> <span class="nc">StaticQuantCustomModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_observed</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">observed_module</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">observed_module</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">observed_module</span><span class="p">,</span> <span class="s1">&#39;activation_post_process&#39;</span><span class="p">)</span>
        <span class="n">observed_module</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">activation_post_process</span> <span class="o">=</span> \
            <span class="n">observed_module</span><span class="o">.</span><span class="n">activation_post_process</span>
        <span class="n">quantized</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">nnq</span><span class="o">.</span><span class="n">Linear</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">observed_module</span><span class="o">.</span><span class="n">linear</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">quantized</span>

<span class="c1">#</span>
<span class="c1"># 示例 API 调用（Eager 模式量化）</span>
<span class="c1">#</span>


<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">CustomModule</span><span class="p">())</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">prepare_custom_config_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;float_to_observed_custom_module_class&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">CustomModule</span><span class="p">:</span> <span class="n">ObservedCustomModule</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="n">convert_custom_config_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;observed_to_quantized_custom_module_class&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">ObservedCustomModule</span><span class="p">:</span> <span class="n">StaticQuantCustomModule</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">m</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_qconfig</span>
<span class="n">mp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">prepare_custom_config_dict</span><span class="o">=</span><span class="n">prepare_custom_config_dict</span><span class="p">)</span>
<span class="c1"># calibration (not shown)</span>
<span class="n">mq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span>
    <span class="n">mp</span><span class="p">,</span> <span class="n">convert_custom_config_dict</span><span class="o">=</span><span class="n">convert_custom_config_dict</span><span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># 示例 API 调用（FX 图模式量化）</span>
<span class="c1">#</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">CustomModule</span><span class="p">())</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_qconfig</span><span class="p">}</span>
<span class="n">prepare_custom_config_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;float_to_observed_custom_module_class&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;static&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="n">CustomModule</span><span class="p">:</span> <span class="n">ObservedCustomModule</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="n">convert_custom_config_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;observed_to_quantized_custom_module_class&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;static&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="n">ObservedCustomModule</span><span class="p">:</span> <span class="n">StaticQuantCustomModule</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="n">mp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize_fx</span><span class="o">.</span><span class="n">prepare_fx</span><span class="p">(</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">,</span> <span class="n">prepare_custom_config_dict</span><span class="o">=</span><span class="n">prepare_custom_config_dict</span><span class="p">)</span>
<span class="c1"># calibration (not shown)</span>
<span class="n">mq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize_fx</span><span class="o">.</span><span class="n">convert_fx</span><span class="p">(</span>
    <span class="n">mp</span><span class="p">,</span> <span class="n">convert_custom_config_dict</span><span class="o">=</span><span class="n">convert_custom_config_dict</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id10">
<h2>量化模型准备（Eager 模式）<a class="headerlink" href="#id10" title="永久链接至标题">#</a></h2>
<p>目前有必要在 Eager 模式量化之前对模型定义进行一些修改。这是因为目前的量化工作是基于一个模块一个模块的。特别地，对于所有量化技术，用户需要：</p>
<ol class="simple">
<li><p>将任何需要输出再量化请求的运算（因此有额外的参数）从函数形式转换为模块形式（例如，使用 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU" title="(在 PyTorch v1.11.0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReLU</span></code></a> 而不是 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="(在 PyTorch v1.11.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.relu()</span></code></a>）。</p></li>
<li><p>通过在子模块上指定 <code class="docutils literal notranslate"><span class="pre">.qconfig</span></code> 属性或指定 <code class="docutils literal notranslate"><span class="pre">qconfig_dict</span></code> 来指定模型的哪些部分需要量化。例如，设置 <code class="docutils literal notranslate"><span class="pre">model.conv1.qconfig</span> <span class="pre">=</span> <span class="pre">None</span></code> 表示 <code class="docutils literal notranslate"><span class="pre">model.conv1</span></code> 层不量化，设置 <code class="docutils literal notranslate"><span class="pre">model.linear1.qconfig</span> <span class="pre">=</span> <span class="pre">custom_qconfig</span></code> 表示 <code class="docutils literal notranslate"><span class="pre">model.linear1</span></code> 将使用 <code class="docutils literal notranslate"><span class="pre">custom_qconfig</span></code> 而不是全局 <code class="docutils literal notranslate"><span class="pre">qconfig</span></code>。</p></li>
</ol>
<p>对于量化激活的静态量化技术，用户还需要做以下工作：</p>
<ol class="simple">
<li><p>指定量化和反量化激活的位置。这是使用 <a class="reference external" href="https://xinetzone.github.io/pytorch-book/api/pytorch/ao/quantization/generated/torch.ao.quantization.stubs.QuantStub.html#torch.ao.quantization.stubs.QuantStub" title="(在 Pytorch Book)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantStub</span></code></a> 和 <a class="reference external" href="https://xinetzone.github.io/pytorch-book/api/pytorch/ao/quantization/generated/torch.ao.quantization.stubs.DeQuantStub.html#torch.ao.quantization.stubs.DeQuantStub" title="(在 Pytorch Book)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeQuantStub</span></code></a> 模块完成的。</p></li>
<li><p>使用 <a class="reference external" href="https://xinetzone.github.io/pytorch-book/api/pytorch/nn/generated/torch.nn.quantized.FloatFunctional.html#torch.nn.quantized.FloatFunctional" title="(在 Pytorch Book)"><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatFunctional</span></code></a> 将需要对量化进行特殊处理的张量运算封装到模块中。例如像 <code class="xref py py-func docutils literal notranslate"><span class="pre">add()</span></code> 和 <code class="xref py py-func docutils literal notranslate"><span class="pre">cat()</span></code> 这样需要特殊处理来确定输出量化参数的运算。</p></li>
<li><p>融合模块：将运算/模块组合成单个模块，获得更高的 accuracy 和性能。这是使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ao.quantization.fuse_modules()</span></code> API 完成的，该 API 接受要融合的模块列表。目前支持以下融合：<code class="docutils literal notranslate"><span class="pre">[Conv,</span> <span class="pre">Relu]</span></code>、 <code class="docutils literal notranslate"><span class="pre">[Conv,</span> <span class="pre">BatchNorm]</span></code>、 <code class="docutils literal notranslate"><span class="pre">[Conv,</span> <span class="pre">BatchNorm,</span> <span class="pre">Relu]</span></code> 和 <code class="docutils literal notranslate"><span class="pre">[Linear,</span> <span class="pre">Relu]</span></code>。</p></li>
</ol>
</section>
<section id="id11">
<h2>最佳实践（已废弃）<a class="headerlink" href="#id11" title="永久链接至标题">#</a></h2>
<p>如果你使用 <code class="docutils literal notranslate"><span class="pre">fbgemm</span></code> 后端，设置观测者的 <code class="docutils literal notranslate"><span class="pre">reduce_range</span></code> 参数为 <code class="docutils literal notranslate"><span class="pre">True</span></code>。该参数通过将量化数据类型的范围减少 1 位来防止某些 int8 指令的溢出。</p>
</section>
<section id="id12">
<h2>数值调试（原型）<a class="headerlink" href="#id12" title="永久链接至标题">#</a></h2>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>数值调试工具是早期的原型，可能会发生变化。</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ao.ns._numeric_suite()</span></code> Eager 模式数值套件</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ao.ns._numeric_suite_fx()</span></code> FX 数值套件</p></li>
</ul>
</section>
<section id="id13">
<h2>常见错误<a class="headerlink" href="#id13" title="永久链接至标题">#</a></h2>
<section id="id14">
<h3>保存和加载量化模型<a class="headerlink" href="#id14" title="永久链接至标题">#</a></h3>
<p>当调用 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="(在 PyTorch v1.11.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> 量化模型，如果你看到如下错误：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ne">AttributeError</span><span class="p">:</span> <span class="s1">&#39;LinearPackedParams&#39;</span> <span class="nb">object</span> <span class="n">has</span> <span class="n">no</span> <span class="n">attribute</span> <span class="s1">&#39;_modules&#39;</span>
</pre></div>
</div>
<p>这是因为直接使用 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.save.html#torch.save" title="(在 PyTorch v1.11.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> 和 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="(在 PyTorch v1.11.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a> 保存和加载量化模型是不支持的。为了保存/加载量化模型，可以使用以下方法：</p>
<ol>
<li><p>保存/加载量化模型 <code class="docutils literal notranslate"><span class="pre">state_dict</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">prepare_orig</span> <span class="o">=</span> <span class="n">prepare_fx</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;&#39;</span> <span class="p">:</span> <span class="n">default_qconfig</span><span class="p">})</span>
<span class="n">prepare_orig</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">quantized_orig</span> <span class="o">=</span> <span class="n">convert_fx</span><span class="p">(</span><span class="n">prepare_orig</span><span class="p">)</span>

<span class="c1"># Save/load using state_dict</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">quantized_orig</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">b</span><span class="p">)</span>

<span class="n">m2</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">prepared</span> <span class="o">=</span> <span class="n">prepare_fx</span><span class="p">(</span><span class="n">m2</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;&#39;</span> <span class="p">:</span> <span class="n">default_qconfig</span><span class="p">})</span>
<span class="n">quantized</span> <span class="o">=</span> <span class="n">convert_fx</span><span class="p">(</span><span class="n">prepared</span><span class="p">)</span>
<span class="n">b</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">quantized</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p>使用 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.save.html#torch.jit.save" title="(在 PyTorch v1.11.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.save()</span></code></a> 和 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.load.html#torch.jit.load" title="(在 PyTorch v1.11.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.load()</span></code></a> 保存/加载脚本化量化模型</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: using the same model M from previous example</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">prepare_orig</span> <span class="o">=</span> <span class="n">prepare_fx</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;&#39;</span> <span class="p">:</span> <span class="n">default_qconfig</span><span class="p">})</span>
<span class="n">prepare_orig</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">quantized_orig</span> <span class="o">=</span> <span class="n">convert_fx</span><span class="p">(</span><span class="n">prepare_orig</span><span class="p">)</span>

<span class="c1"># save/load using scripted model</span>
<span class="n">scripted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">quantized_orig</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">scripted</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">b</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">scripted_quantized</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id15">
<h3>传递非量化的张量到量化的核<a class="headerlink" href="#id15" title="永久链接至标题">#</a></h3>
<p>如果你看到类似的错误：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">Could</span> <span class="ow">not</span> <span class="n">run</span> <span class="s1">&#39;quantized::some_operator&#39;</span> <span class="k">with</span> <span class="n">arguments</span> <span class="kn">from</span> <span class="nn">the</span> <span class="s1">&#39;CPU&#39;</span> <span class="n">backend</span><span class="o">...</span>
</pre></div>
</div>
<p>这意味着你试图传递非量化的张量给量化的核。常见的解决方法是使用 <code class="docutils literal notranslate"><span class="pre">torch.quantization.QuantStub</span></code> 来量化张量。这需要在 Eager 模式量化中手动完成。e2e 的例子：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantStub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># during the convert step, this will be replaced with a</span>
        <span class="c1"># `quantize_per_tensor` call</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="id16">
<h3>传递量化的张量到非量化的核<a class="headerlink" href="#id16" title="永久链接至标题">#</a></h3>
<p>如果你看到类似的错误：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">Could</span> <span class="ow">not</span> <span class="n">run</span> <span class="s1">&#39;aten::thnn_conv2d_forward&#39;</span> <span class="k">with</span> <span class="n">arguments</span> <span class="kn">from</span> <span class="nn">the</span> <span class="s1">&#39;QuantizedCPU&#39;</span> <span class="n">backend</span><span class="o">.</span>
</pre></div>
</div>
<p>这意味着你试图传递量化的张量给非量化的核。常见的解决方法是使用 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.quantization.DeQuantStub.html#torch.quantization.DeQuantStub" title="(在 PyTorch v1.11.0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.quantization.DeQuantStub</span></code></a> 反量化张量。这需要在 Eager 模式量化中手动完成。e2e 的例子：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantStub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># this module will not be quantized (see `qconfig = None` logic below)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dequant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">DeQuantStub</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># during the convert step, this will be replaced with a</span>
        <span class="c1"># `quantize_per_tensor` call</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># during the convert step, this will be replaced with a</span>
        <span class="c1"># `dequantize` call</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">some_qconfig</span>
<span class="c1"># turn off quantization for conv2</span>
<span class="n">m</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="index.html" title="上一页 页">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">上一页</p>
            <p class="prev-next-title">PyTorch 量化教程</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="best-practice.html" title="下一页 页">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">下一页</p>
        <p class="prev-next-title">PTQ 与 QAT 量化实践（PyTorch）</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By xinetzone<br/>
  
      &copy; Copyright 2022, xinetzone.<br/>
    Last updated on 2022-03-26, 10:27:19.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>