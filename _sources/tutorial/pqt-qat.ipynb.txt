{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTQ 与 QAT 实践\n",
    "\n",
    "本文主要介绍如何使用 PyTorch 将浮点模型转换为 PTQ 或者 QAT 模型。\n",
    "\n",
    "## 背景\n",
    "\n",
    "{guilabel}`目标`：快速将浮点模型转换为 PTQ 或者 QAT 模型。\n",
    "\n",
    "### 读者\n",
    "\n",
    "本教程适用于会使用 PyTorch 编写 CNN 等模块的的算法工程师。\n",
    "\n",
    "### 环境配置\n",
    "\n",
    "本文使用 Python 3.10.0 （其他版本请自测），暂时仅 Linux 平台被测试。\n",
    "\n",
    "为了提供一致的量化工具接口，我们使用 Python 包 `torchq`。\n",
    "\n",
    "本地载入临时 `torchq` 包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod import torchq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "本文使用 `torchq` 的 `'0.0.1-alpha'` 版本。\n",
    "```\n",
    "\n",
    "更方便的是：使用 `pip` 安装：\n",
    "\n",
    "```shell\n",
    "pip install torchq==0.0.1-alpha\n",
    "```\n",
    "\n",
    "接着，便可以直接导入：\n",
    "\n",
    "```python\n",
    "import torchq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 `torch` 和 `torchvision` 的版本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.11.0 \n",
      "torchvision: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f'torch: {torch.__version__} \\n'\n",
    "      f'torchvision: {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置一些警告配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module='.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    module='torch.ao.quantization'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述：PQT 与 QAT\n",
    "\n",
    "参考：[量化](https://pytorch.org/docs/master/quantization.html)\n",
    "\n",
    "`训练后量化`\n",
    ":   简称 PTQ（Post Training Quantization）：权重量化，激活量化，需要借助数据在训练后进行校准。\n",
    "\n",
    "`静态量化感知训练`\n",
    ":   简称 QAT（static quantization aware training）：权重量化，激活量化，在训练过程中的量化数值进行建模。\n",
    "\n",
    "`浮点模型`\n",
    ":   模型的 **权重** 和 **激活** 均为浮点类型（如 {data}`torch.float32`, {data}`torch.float64`）。\n",
    "\n",
    "`量化模型`\n",
    ":   模型的 **权重** 和 **激活** 均为量化类型（如 {data}`torch.qint32`, {data}`torch.qint8`, {data}`torch.quint8`, {data}`torch.quint2x4`, {data}`torch.quint4x2`）。\n",
    "\n",
    "\n",
    "下面举例说明如何将浮点模型转换为量化模型。\n",
    "\n",
    "为了方便说明定义如下模块：\n",
    "\n",
    "```{rubric} 定义简单的浮点模块\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        '''提供便捷函数'''\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x= self._forward_impl(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 定义可量化模型\n",
    "```\n",
    "\n",
    "将浮点模块 `M` 转换为可量化模块 `QM`（量化流程的最关键的一步）。\n",
    "\n",
    "```{rubric} 定义可量化模块\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "\n",
    "class QM(M):\n",
    "    '''\n",
    "    Args:\n",
    "        is_print: 为了测试需求，打印一些信息\n",
    "    '''\n",
    "    def __init__(self, is_print: bool=False):\n",
    "        super().__init__()\n",
    "        self.is_print = is_print\n",
    "        self.quant = QuantStub() # 将张量从浮点转换为量化\n",
    "        self.dequant = DeQuantStub() # 将张量从量化转换为浮点\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # 手动指定张量将在量化模型中从浮点模块转换为量化模块的位置\n",
    "        x = self.quant(x)\n",
    "        if self.is_print:\n",
    "            print('量化前的类型：', x.dtype)\n",
    "        x = self._forward_impl(x)\n",
    "        if self.is_print:\n",
    "            print('量化中的类型：',x.dtype)\n",
    "        # 在量化模型中手动指定张量从量化到浮点的转换位置\n",
    "        x = self.dequant(x)\n",
    "        if self.is_print:\n",
    "            print('量化后的类型：', x.dtype)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单测试前向过程的激活数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4) # 输入的数据\n",
    "\n",
    "m = QM(is_print=True)\n",
    "x = m(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看权重的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.conv.weight.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，此时模块 `m` 是浮点模块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTQ 简介\n",
    "\n",
    "当内存带宽和计算空间都很重要时，通常会使用训练后量化，而 CNN 就是其典型的用例。训练后量化对模型的 **权重** 和 **激活** 进行量化。它在可能的情况下将 **激活** 融合到前面的层中。它需要用具有代表性的数据集进行 **校准**，以确定激活的最佳量化参数。\n",
    "\n",
    "```{rubric} 示意图\n",
    "```\n",
    "\n",
    "```\n",
    "# 原始模型\n",
    "# 全部的张量和计算均在浮点上进行\n",
    "previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n",
    "                    /\n",
    "    linear_weight_fp32\n",
    "\n",
    "# 静态量化模型\n",
    "# weights 和 activations 在 int8 上\n",
    "previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n",
    "                    /\n",
    "  linear_weight_int8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接创建浮点模块的实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建浮点模型实例\n",
    "model_fp32 = QM(is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使 PTQ 生效，必须将模型设置为 `eval` 模式：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看此时的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'\n",
    "      f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 定义观测器\n",
    "```\n",
    "\n",
    "赋值实例变量 `qconfig`，其中包含关于要附加哪种观测器的信息：\n",
    "\n",
    "- 使用 `'fbgemm'` 进行服务端推理，使用 `'qnnpack'` 进行移动端推理。\n",
    "- 其他量化配置，如选择对称或非对称量化和 `MinMax` 或 `L2Norm` 校准技术，可以在这里指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看此时的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'\n",
    "      f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 融合激活层\n",
    "```\n",
    "\n",
    "在适用的地方，融合 activation 到前面的层（这需要根据模型架构手动完成）。常见的融合包括 `conv + relu` 和 `conv + batchnorm + relu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,\n",
    "                                                      [['conv', 'relu']])\n",
    "                                                    \n",
    "model_fp32_fused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 `model_fp32_fused` 中 `ConvReLU2d` 融合 `model_fp32` 的两个层 `conv` 和 `relu`。\n",
    "\n",
    "查看此时的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32_fused(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'\n",
    "      f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 启用观测器\n",
    "```\n",
    "\n",
    "在融合后的模块中启用观测器，用于在校准期间观测激活（activation）张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 校准准备好的模型\n",
    "```\n",
    "校准准备好的模型，以确定量化参数的激活在现实世界的设置，校准具有代表性的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32_prepared(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'\n",
    "      f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 模型转换\n",
    "```\n",
    "\n",
    "```{note}\n",
    "量化权重，计算和存储每个激活张量要使用的尺度（scale）和偏差（bias）值，并用量化实现替换关键算子。\n",
    "```\n",
    "\n",
    "转换已校准好的模型为量化模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): QuantizedConvReLU2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.019093120470643044, zero_point=0)\n",
       "  (relu): Identity()\n",
       "  (quant): Quantize(scale=tensor([0.0368]), zero_point=tensor([72]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "model_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看权重的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.qint8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8.conv.weight().dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出此时权重的元素大小为 1 字节，而不是 FP32 的 4 字节："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8.conv.weight().element_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行模型，相关的计算将在 {data}`torch.qint8` 中发生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前的类型： torch.quint8\n",
      "量化中的类型： torch.quint8\n",
      "量化后的类型： torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model_int8(input_fp32)\n",
    "res.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要了解更多关于量化意识训练的信息，请参阅 [QAT 教程](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QAT 概述\n",
    "\n",
    "与其他量化方法相比，QAT 在 **训练过程中** 模拟量化的效果，可以获得更高的 accuracy。在训练过程中，所有的计算都是在浮点上进行的，使用 fake_quant 模块通过夹紧和舍入的方式对量化效果进行建模，模拟 INT8 的效果。模型转换后，权值和激活被量化，激活在可能的情况下被融合到前一层。它通常与 CNN 一起使用，与 PTQ 相比具有更高的 accuracy。\n",
    "\n",
    "\n",
    "```{rubric} 示意图\n",
    "```\n",
    "\n",
    "```\n",
    "# 原始模型\n",
    "# 全部张量和计算均在浮点上\n",
    "previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n",
    "                      /\n",
    "    linear_weight_fp32\n",
    "\n",
    "# 在训练过程中使用 fake_quants 建模量化数值\n",
    "previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n",
    "                           /\n",
    "   linear_weight_fp32 -- fq\n",
    "\n",
    "# 量化模型\n",
    "# weights 和 activations 在 int8 上\n",
    "previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n",
    "                     /\n",
    "   linear_weight_int8\n",
    "```\n",
    "\n",
    "定义比 `M` 稍微复杂一点的浮点模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M2(M):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = torch.nn.BatchNorm2d(1)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        '''提供便捷函数'''\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样需要定义可量化模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QM2(M2, QM):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建浮点模型实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM2(\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建模型实例\n",
    "model_fp32 = QM2()\n",
    "model_fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型必须设置为训练模式，以便 QAT 可用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加量化配置（与 PTQ 相同相似）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 融合 QAT 模块\n",
    "```\n",
    "\n",
    "QAT 的模块融合与 PTQ 相同相似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import fuse_modules_qat\n",
    "\n",
    "model_fp32_fused = fuse_modules_qat(model_fp32,\n",
    "                                    [['conv', 'bn', 'relu']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 准备 QAT 模型\n",
    "```\n",
    "\n",
    "这将在模型中插入观测者和伪量化模块，它们将在校准期间观测权重和激活的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 训练 QAT 模型\n",
    "```\n",
    "\n",
    "```python\n",
    "# 下文会编写实际的例子，此处没有显示\n",
    "training_loop(model_fp32_prepared)\n",
    "```\n",
    "\n",
    "将观测到的模型转换为量化模型。需要：\n",
    "\n",
    "- 量化权重，计算和存储用于每个激活张量的尺度（scale）和偏差（bias）值，\n",
    "- 在适当的地方融合模块，并用量化实现替换关键算子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行模型，相关的计算将在 {data}`torch.qint8` 中发生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要了解更多关于量化意识训练的信息，请参阅 [QAT 教程](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)。\n",
    "\n",
    "### PTQ/QAT 统一的量化流程\n",
    "\n",
    "PTQ 和 QAT 的量化流程十分相似，为了统一接口，可以使用 `torchvision` 提供的函数 {func}`~torchvision.models.quantization.utils._fuse_modules`。\n",
    "\n",
    "下面利用函数 {func}`~torchvision.models.quantization.utils._fuse_modules` 可量化模块 `QM2`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch.ao.quantization import fuse_modules, fuse_modules_qat\n",
    "from torch.ao.quantization import get_default_qconfig, get_default_qat_qconfig\n",
    "from torch.ao.quantization import quantize, quantize_qat\n",
    "\n",
    "def _fuse_modules(\n",
    "    model: nn.Module, modules_to_fuse: list[str] | list[list[str]], is_qat: bool | None, **kwargs: Any\n",
    "):\n",
    "    if is_qat is None:\n",
    "        is_qat = model.training\n",
    "    method = fuse_modules_qat if is_qat else fuse_modules\n",
    "    return method(model, modules_to_fuse, **kwargs)\n",
    "\n",
    "\n",
    "class QM3(QM2):\n",
    "    '''可量化模型\n",
    "    Args:\n",
    "        is_qat: 是否使用 QAT 模式\n",
    "    '''\n",
    "    def __init__(self, is_qat: bool | None = None, backend='fbgemm'):\n",
    "        super().__init__()\n",
    "        self.is_qat = is_qat\n",
    "        # 定义观测器\n",
    "        if is_qat:\n",
    "            self.train()\n",
    "            self.qconfig = get_default_qat_qconfig(backend)\n",
    "        else:\n",
    "            self.eval()\n",
    "            self.qconfig = get_default_qconfig(backend)\n",
    "\n",
    "    def fuse_model(self) -> None:\n",
    "        '''模块融合'''\n",
    "        if self.is_qat:\n",
    "            modules_to_fuse = ['bn', 'relu']\n",
    "        else:\n",
    "            modules_to_fuse = ['conv', 'bn', 'relu']\n",
    "        return _fuse_modules(self,\n",
    "                      modules_to_fuse,\n",
    "                      self.is_qat,\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了可量化模块 `QM3`，可以十分便利的切换 PTQ/QAT了。\n",
    "\n",
    "比如，PTQ，可以这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fn(model, num_epochs):\n",
    "    for _ in range(num_epochs):\n",
    "        input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "        model(input_fp32)\n",
    "\n",
    "num_epochs = 10\n",
    "ptq_model = QM3(is_qat=False)\n",
    "model_fused = ptq_model.fuse_model()\n",
    "quanted_model = quantize(model_fused, run_fn, [num_epochs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QAT 可以这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "qat_model = QM3(is_qat=True)\n",
    "model_fused = qat_model.fuse_model()\n",
    "quanted_model = quantize_qat(model_fused, run_fn, [num_epochs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTQ/QAT 量化策略\n",
    "\n",
    "对于 PTQ 和 QAT 技术，除了上述讨论的策略，还需要了解：\n",
    "\n",
    "1. 将任何需要输出再量化请求的运算（因此有额外的参数）从函数形式转换为模块形式（例如，使用 {class}`torch.nn.ReLU` 而不是 {func}`torch.nn.functional.relu`）。\n",
    "1. 通过在子模块上指定 `.qconfig` 属性或指定 `qconfig_dict` 来指定模型的哪些部分需要量化。例如，设置 `model.conv1.qconfig = None` 表示 `model.conv1` 层不量化，设置 `model.linear1.qconfig = custom_qconfig` 表示 `model.linear1` 将使用 `custom_qconfig` 而不是全局 `qconfig`。\n",
    "\n",
    "对于量化激活的静态量化技术，用户还需要做以下工作：\n",
    "\n",
    "1. 指定量化和反量化激活的位置。这是使用 {class}`~torch.ao.quantization.stubs.QuantStub` 和 {class}`~torch.ao.quantization.stubs.DeQuantStub` 模块完成的。\n",
    "1. 使用 {class}`~torch.nn.quantized.FloatFunctional` 将需要对量化进行特殊处理的张量运算封装到模块中。例如像 {func}`add` 和 {func}`cat` 这样需要特殊处理来确定输出量化参数的运算。\n",
    "1. 融合模块：将运算/模块组合成单个模块，获得更高的 accuracy 和性能。这是使用 {func}`~torch.ao.quantization.fuse_modules.fuse_modules` API 完成的，该 API 接受要融合的模块列表。目前支持以下融合：`[Conv, Relu]`、 `[Conv, BatchNorm]`、 `[Conv, BatchNorm, Relu]` 和 `[Linear, Relu]`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTQ 和 QAT 的模块融合\n",
    "\n",
    "可以看出 PTQ 和 QAT 需要用户自定义的内容主要集中在： **模块融合**。\n",
    "\n",
    "{func}`~torchvision.models.quantization.utils._fuse_modules` 提供了 {func}`~torch.ao.quantization.fuse_modules.fuse_modules` 和 {func}`~torch.ao.quantization.fuse_modules.fuse_modules_qat` 的统一接口。下面以 MobileNetV2 为例，简述如何使用 {func}`~torchvision.models.quantization.utils._fuse_modules` 函数定制可量化的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''参考 torchvision/models/quantization/mobilenetv2.py\n",
    "'''\n",
    "from typing import Any\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "from torchvision._internally_replaced_utils import load_state_dict_from_url\n",
    "from torchvision.ops.misc import ConvNormActivation\n",
    "from torchvision.models.quantization.utils import _fuse_modules, _replace_relu, quantize_model\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "from torchvision.models.mobilenetv2 import InvertedResidual, MobileNetV2, model_urls\n",
    "\n",
    "quant_model_urls = {\n",
    "    \"mobilenet_v2_qnnpack\": \"https://download.pytorch.org/models/quantized/mobilenet_v2_qnnpack_37f702c5.pth\"\n",
    "}\n",
    "\n",
    "\n",
    "class QuantizableInvertedResidual(InvertedResidual):\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.use_res_connect:\n",
    "            return self.skip_add.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "    def fuse_model(self, is_qat: bool | None = None) -> None:\n",
    "        for idx in range(len(self.conv)):\n",
    "            if type(self.conv[idx]) is nn.Conv2d:\n",
    "                _fuse_modules(self.conv,\n",
    "                              [str(idx),\n",
    "                               str(idx + 1)],\n",
    "                              is_qat,\n",
    "                              inplace=True)\n",
    "\n",
    "\n",
    "class QuantizableMobileNetV2(MobileNetV2):\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "\n",
    "        Args:\n",
    "           继承自浮点 MobileNetV2 的参数\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.quant(x)\n",
    "        x = self._forward_impl(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self, is_qat: bool | None) -> None:\n",
    "        for m in self.modules():\n",
    "            if type(m) is ConvNormActivation:\n",
    "                _fuse_modules(m, [\"0\", \"1\", \"2\"], is_qat, inplace=True)\n",
    "            if type(m) is QuantizableInvertedResidual:\n",
    "                m.fuse_model(is_qat)\n",
    "\n",
    "\n",
    "def mobilenet_v2(\n",
    "    pretrained: bool = False,\n",
    "    progress: bool = True,\n",
    "    quantize: bool = False,\n",
    "    **kwargs: Any,\n",
    ") -> QuantizableMobileNetV2:\n",
    "    \"\"\"\n",
    "    从 `MobileNetV2：反向残差和线性瓶颈 <https://arxiv.org/abs/1801.04381>`_ 构建 MobileNetV2 架构。\n",
    "\n",
    "    注意，quantize = True 返回具有 8 bit 权值的量化模型。量化模型只支持推理并在 CPU 上运行。\n",
    "    目前还不支持 GPU 推理\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): 如果为 True，返回在 ImageNet 上训练过的模型。\n",
    "        progress (bool): 如果为 True，则显示下载到标准错误的进度条\n",
    "        quantize(bool): 如果为 True，则返回量化模型，否则返回浮点模型\n",
    "    \"\"\"\n",
    "    model = QuantizableMobileNetV2(block=QuantizableInvertedResidual, **kwargs)\n",
    "    _replace_relu(model)\n",
    "\n",
    "    if quantize:\n",
    "        # TODO use pretrained as a string to specify the backend\n",
    "        backend = \"qnnpack\"\n",
    "        quantize_model(model, backend)\n",
    "    else:\n",
    "        assert pretrained in [True, False]\n",
    "\n",
    "    if pretrained:\n",
    "        if quantize:\n",
    "            model_url = quant_model_urls[\"mobilenet_v2_\" + backend]\n",
    "        else:\n",
    "            model_url = model_urls[\"mobilenet_v2\"]\n",
    "\n",
    "        state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTQ 实战\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAT 实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78526419bf48930935ba7e23437b2460cb231485716b036ebb8701887a294fa8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('torchx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
